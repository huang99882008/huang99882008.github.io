<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>llm学习笔记 | Maxwell's Blog</title><meta name="author" content="maxwell"><meta name="copyright" content="maxwell"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="ai时代，个人学习还重要吗？大佬们都在学，不学习还干嘛呢？ Huggingface学习目标  使用huggingface提供的模型和transformers api完成各类推理任务  transformers使用 huggingface中autoXXX的类，都是通过from_pretrained()函数，传入模型名字字符串自动加载对应的模型，免去用户手动导入特定的模型类（模型实在太多了） 模型被下">
<meta property="og:type" content="article">
<meta property="og:title" content="llm学习笔记">
<meta property="og:url" content="http://huangshuai.top/2025/03/21/geektime-llm-note/index.html">
<meta property="og:site_name" content="Maxwell&#39;s Blog">
<meta property="og:description" content="ai时代，个人学习还重要吗？大佬们都在学，不学习还干嘛呢？ Huggingface学习目标  使用huggingface提供的模型和transformers api完成各类推理任务  transformers使用 huggingface中autoXXX的类，都是通过from_pretrained()函数，传入模型名字字符串自动加载对应的模型，免去用户手动导入特定的模型类（模型实在太多了） 模型被下">
<meta property="og:locale">
<meta property="og:image" content="http://huangshuai.top/images/avatar.png">
<meta property="article:published_time" content="2025-03-21T09:18:03.000Z">
<meta property="article:modified_time" content="2025-05-29T12:20:20.029Z">
<meta property="article:author" content="maxwell">
<meta property="article:tag" content="llm">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://huangshuai.top/images/avatar.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "llm学习笔记",
  "url": "http://huangshuai.top/2025/03/21/geektime-llm-note/",
  "image": "http://huangshuai.top/images/avatar.png",
  "datePublished": "2025-03-21T09:18:03.000Z",
  "dateModified": "2025-05-29T12:20:20.029Z",
  "author": [
    {
      "@type": "Person",
      "name": "maxwell",
      "url": "http://huangshuai.top/"
    }
  ]
}</script><link rel="shortcut icon" href="/images/favicon.ico"><link rel="canonical" href="http://huangshuai.top/2025/03/21/geektime-llm-note/index.html"><link rel="preconnect"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="/"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: '/',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'llm学习笔记',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background: linear-gradient(20deg, #A0A0A0, #E6E6E6,#1C223C, #3A3F5B);"></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/images/Starry_Sky.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Maxwell's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">llm学习笔记</span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">llm学习笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-03-21T09:18:03.000Z" title="Created 2025-03-21 17:18:03">2025-03-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-05-29T12:20:20.029Z" title="Updated 2025-05-29 20:20:20">2025-05-29</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p><em><strong>ai时代，个人学习还重要吗？大佬们都在学，不学习还干嘛呢？</strong></em></p>
<h1 id="Huggingface"><a href="#Huggingface" class="headerlink" title="Huggingface"></a>Huggingface</h1><h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2><ul>
<li><input checked="" disabled="" type="checkbox"> 使用huggingface提供的模型和transformers api完成各类推理任务</li>
</ul>
<h2 id="transformers使用"><a href="#transformers使用" class="headerlink" title="transformers使用"></a>transformers使用</h2><ul>
<li>huggingface中autoXXX的类，都是通过from_pretrained()函数，传入模型名字字符串自动加载对应的模型，免去用户手动导入特定的模型类（模型实在太多了）</li>
<li>模型被下载到(ubuntu): .cache&#x2F;huggingface&#x2F;hub&#x2F;model-name&#x2F;下的blob目录下，snapshot下存放的是链接到模型文件的softlink，可以看到有<code>config.json  |  model.safetensors | tokenizer_config.json  |  vocab.txt</code>这些文件，vocab.txt是词汇表，model.safetensor是模型参数,safetensor是huggingface的参数文件格式，比pytorch的bin更安全（不会被恶意代码注入），可以用<code>pip install safetensors</code>这个lib打开。</li>
</ul>
<h2 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h2><ul>
<li>文本分类模型不一定支持中文，比如默认的<strong>distilbert-base-uncased-finetuned-sst-2-english</strong>模型是个忽略大小写的英文模型</li>
<li>使用pipeline(“text-classification”)，默认调用的是sentiment分类，其他分类任务需要补充model参数</li>
</ul>
<h1 id="MATH"><a href="#MATH" class="headerlink" title="MATH"></a>MATH</h1><h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.3blue1brown.com/">3blue1brown</a>可视化学习ai相关数学知识，原谅自己本科阶段。</li>
</ul>
<h1 id="PROMPT-ENGINEERING"><a href="#PROMPT-ENGINEERING" class="headerlink" title="PROMPT ENGINEERING"></a>PROMPT ENGINEERING</h1><h2 id="资料-1"><a href="#资料-1" class="headerlink" title="资料"></a>资料</h2><h3 id="paper"><a href="#paper" class="headerlink" title="paper"></a>paper</h3><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2201.11903">“Chain of Thought Prompting Elicits Reasoning in Large Language Models” (CoT)</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.11171">“Self-Consistency Improves Chain of Thought Reasoning in Large Language Models”</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.10601">“Tree of Thoughts: Deliberate Problem Solving with LLMs” (ToT)</a></li>
</ul>
<h1 id="RAG"><a href="#RAG" class="headerlink" title="RAG"></a>RAG</h1><h2 id="资料-2"><a href="#资料-2" class="headerlink" title="资料"></a>资料</h2><h3 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h3><ul>
<li><a target="_blank" rel="noopener" href="https://llamahub.ai/">llamahub.aiz</a></li>
</ul>
<h1 id="FINE-TUNE"><a href="#FINE-TUNE" class="headerlink" title="FINE-TUNE"></a>FINE-TUNE</h1><h2 id="资料google’s-saif"><a href="#资料google’s-saif" class="headerlink" title="资料google’s saif"></a>资料google’s saif</h2><ul>
<li><a target="_blank" rel="noopener" href="https://geek-agi.feishu.cn/wiki/B9rYwwg6xidZYJkbrlscxTQFnOc">极客时间分享的wiki</a></li>
<li><a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">图解transformer</a></li>
</ul>
<h2 id="工具-1"><a href="#工具-1" class="headerlink" title="工具"></a>工具</h2><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h3 id="稀疏向量"><a href="#稀疏向量" class="headerlink" title="稀疏向量"></a>稀疏向量</h3><p>“向量稀疏”指的是一个向量中大多数元素为零，只有少数元素是非零值的情况。在机器学习和自然语言处理中，向量稀疏通常是由于向量维度很高，而实际数据中只包含少数非零元素的结果。反义词是“稠密向量”（Dense Vector）：一个向量中大多数元素是非零值的情况。</p>
<h3 id="One-Hot编码"><a href="#One-Hot编码" class="headerlink" title="One-Hot编码"></a>One-Hot编码</h3><p>One-Hot编码是一种常用的特征表示方法，用于将离散特征转换为向量表示。把某个“类别”表示成一个 全是 0，只有一个位置是 1 的向量。</p>
<ul>
<li>🤔 为什么要用 One-Hot 编码？<br>因为机器学习模型（尤其是线性模型、神经网络）不懂“类别”这些概念。<br>如果你直接用字符串，模型是无法处理的。<br>One-Hot 把“类别”变成了“数字向量”，使得模型可以进行数学计算。</li>
<li>⚠️ 缺点：<br>维度高：如果你有上万个类别（比如 NLP 里的词汇），One-Hot 向量会非常稀疏（大部分都是 0）。<br>不包含语义关系：比如“北京”和“上海”的 One-Hot 向量之间完全没有距离或关联。<br>因此，在复杂应用中（如 NLP），我们通常用 词嵌入（word embeddings） 来代替 One-Hot，比如 Word2Vec、BERT 等。</li>
</ul>
<h3 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h3><p>将文本切分为一个个子词，然后再映射为词汇表中的ID，就是token。<br>词汇表中通常已经把常见的单个字符包含了，所以正常情况下，所有文本都可以顺利被tokenization，如何遇到未知字符，会有个特殊的UNK token表示。<br>gpt3.5有5万多个词</p>
<h3 id="词嵌入（Word-Embedding）"><a href="#词嵌入（Word-Embedding）" class="headerlink" title="词嵌入（Word Embedding）"></a>词嵌入（Word Embedding）</h3><p>词嵌入是一种将单词表示为向量的技术，这些向量捕捉单词之间的语义关系。<br>维度低：每个词被映射到一个低维向量空间，通常是 50 到 300 维。<br>捕捉语义关系：词嵌入向量之间的距离可以表示单词之间的语义关系。</p>
<ul>
<li><p>🧩 词嵌入怎么来的？有两大类方法：</p>
<p>  <strong>统计方法,比如Word2Vec、GloVe 等</strong>。</p>
<p>  <strong>深度学习方法,比如 BERT、GPT 等。</strong></p>
</li>
<li><p>⚠️ 缺点：<br>需要大量的训练数据：词嵌入需要大量的文本数据来训练，才能捕捉单词之间的语义关系。</p>
</li>
</ul>
<h3 id="激活函数（activation-function）"><a href="#激活函数（activation-function）" class="headerlink" title="激活函数（activation function）"></a>激活函数（activation function）</h3><p>激活函数是神经网络中用于<strong>引入非线性</strong>变换的函数。它们将输入信号转换为输出信号，使得神经网络可以学习复杂的非线性关系。</p>
<ul>
<li>🤔 为什么要用激活函数？<br>如果没有激活函数，神经网络每一层只是：<br><img src="/images/post_images/2025-04-18-11-16-44.png"><br>这其实就是线性变换，多个线性变换堆叠起来，本质上仍然是一个线性函数（向量投影），虽然也能反映一定的关系，但无法表示复杂的关系。<br>激活函数引入非线性变换，可以使神经网络学习到更复杂的模式。<br><img src="/images/post_images/2025-04-18-11-18-42.png"><br><img src="/images/post_images/2025-04-18-11-19-32.png"><br><img src="/images/post_images/2025-04-18-11-20-28.png"><br><img src="/images/post_images/2025-04-18-11-21-29.png"><br><img src="/images/post_images/2025-04-18-11-22-04.png"><br><img src="/images/post_images/2025-04-18-11-23-00.png"></li>
</ul>
<h3 id="梯度下降（Gradient-Descent）"><a href="#梯度下降（Gradient-Descent）" class="headerlink" title="梯度下降（Gradient Descent）"></a>梯度下降（Gradient Descent）</h3><p>对于一维函数，梯度等价于导数，对于多维函数，梯度就是一个向量，它的方向指向函数值增加最快的方向。<br>而梯度下降，就是沿着梯度的反方向，不断地更新参数，使得函数值（比如 Loss 越小越好）不断减小。<br>在工程中，梯度下降通过 <strong>loss.backward() + optimizer.step()</strong> 来实现。框架帮你自动做了链式求导和参数更新，你只需要选好模型 → 选好 loss → 调 optimizer → 调训练循环</p>
<ul>
<li><strong>loss.backward()</strong> 会自动用链式法则计算出所有参数的梯度（这就是反向传播）</li>
<li><strong>optimizer.step()</strong> 会调用优化器，比如 SGD&#x2F;Adam，用梯度来更新模型参数（相当于“沿负梯度方向走一步”），可以用公式<strong>W &#x3D; W - 学习率 × 梯度</strong>来理解。<br><img src="/images/post_images/2025-04-18-14-28-52.png"></li>
</ul>
<h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><p>神经网络的结构是分层的，每一层有一定数量的神经元，这个数量通常是训练之前就已经确定的。且神经元的数量要与输入向量的维度保持一致。<br>假设有一个输入向量(相当于一个样本，或者理解成一个token经过嵌入的向量)：</p>
<blockquote>
<p>x &#x3D; [x₁, x₂, x₃, …, xₙ] ∈ ℝⁿ</p>
</blockquote>
<p>当前layer有m个神经元，每个神经元的weight是一个向量，这个向量的维度和输入向量的维度必须相同：</p>
<blockquote>
<p>wᵢ &#x3D; [wᵢ₁, wᵢ₂, wᵢ₃,…, wᵢₙ] ∈ ℝⁿ</p>
</blockquote>
<p>每个神经元的bias是一个标量：</p>
<blockquote>
<p>bᵢ ∈ ℝ</p>
</blockquote>
<p>输入经过每个神经元进行计算(点积加上偏置，再输入到激活函数里转成概率)，输出一个标量：</p>
<blockquote>
<p>yᵢ &#x3D; f(wᵢ * x + bᵢ)</p>
</blockquote>
<p>多个神经元同时接收输入，进行并行计算，每个神经元输出一个标量，组合在一起就形成了一个向量，多个样本就形成了一个矩阵。</p>
<h3 id="隐藏状态（Hidden-State）"><a href="#隐藏状态（Hidden-State）" class="headerlink" title="隐藏状态（Hidden State）"></a>隐藏状态（Hidden State）</h3><p>每一层的计算结果输出的向量通常被称为 隐藏状态（Hidden State），隐藏状态向量的维度和上一层的神经元数量相同。<br>在神经网络的上下文中，隐藏状态是指每一层神经网络在进行计算后得到的中间结果。<br><strong>前向传播</strong>：这个中间结果是输入数据经过该层计算得到的特征表示。在网络的传递过程中逐层向后传播。<br><strong>反向传播</strong>：前向传播结束后，最后一层的输出的隐藏状态向量传递到损失函数loss中，得到损失值。损失值通过反向传播计算出梯度，梯度会沿着每一层的隐藏状态进行反向传递。通过计算这些梯度，更新每一层的权重。</p>
<h3 id="encoder-decoder"><a href="#encoder-decoder" class="headerlink" title="encoder &amp; decoder"></a>encoder &amp; decoder</h3><p>神经网络中的两个不同“子系统”，分别负责「理解输入」和「生成输出」。<br>可以理解成模型的两个不同模块，每个模块负责不同的任务，都包含了多个layer。<br>🤖 <strong>在 Transformer 中的例子：</strong><br>✅ Encoder（编码器）结构<br>由多个 Encoder Layer 组成，通常是 6 层（也可能是 12、24 层，视模型大小而定）：<br>每一层都包括：&#x2F;home&#x2F;huangshuai&#x2F;code&#x2F;my-github-blog&#x2F;source&#x2F;_posts<br>多头自注意力机制（Multi-Head Self Attention）<br>前馈神经网络（Feed Forward Network）<br>残差连接 + LayerNorm</p>
<p>✅ Decoder（解码器）结构<br>同样由多个 Decoder Layer 组成：<br>每一层包括：<br>Masked 多头自注意力机制（不能看到未来）<br>编码器-解码器注意力机制（Attention over encoder outputs）<br>前馈神经网络<br>残差连接 + LayerNorm</p>
<h3 id="注意力机制attention"><a href="#注意力机制attention" class="headerlink" title="注意力机制attention"></a>注意力机制attention</h3><p>注意力机制是一种用于解决序列模型中的长距离依赖问题的技术。<br>本质是根据一个输入（query），在多个候选项(key)中找出与它最相关的信息(value)。<br>每个token的嵌入向量经过与qkv权重的线性变换后，得到qkv三个向量。</p>
<blockquote>
<p>A(q, k, v) &#x3D; Σ p(a(kᵢ, q)) * vᵢ</p>
</blockquote>
<p>其中，p(a(kᵢ, q)) 是一个注意力分数，它表示当前 token 对其他 token 的注意力权重。a(kᵢ, q)：表示 q 和 kᵢ 之间的相似度（打分函数，通常是点积 q ⋅ kᵢ），p函数通常是一个softmax函数，用于将分数归一化到 0 到 1 之间。</p>
<p>多头注意力机制：<br>输入的向量维度被切分成多个子向量，每个子向量分别进行注意力计算，最后将多个子向量的结果拼接起来。<br>这么做有什么用？<br>每个头能从不同的视角看输入，每个注意力头关注的角度是训练过程中自动涌现的，哪些头学会看长距离、哪些头学会看位置、哪些头学会看同义词关系……这些是自然涌现的。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://huangshuai.top">maxwell</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://huangshuai.top/2025/03/21/geektime-llm-note/">http://huangshuai.top/2025/03/21/geektime-llm-note/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/llm/">llm</a></div><div class="post-share"><div class="social-share" data-image="/images/avatar.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="/" media="print" onload="this.media='all'"><script src="/" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/03/07/deep-dive-into-llms-from-karpathy/" title="笔记:深入了解LLMs"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">笔记:深入了解LLMs</div></div><div class="info-2"><div class="info-item-1">介绍llm的一些入门扫盲知识，🤓️ai大神karpathy总结的比较全面，看完有些收获记录下。视频地址：Deep Dive into LLMs like ChatGPT is on under-the hood fundamentals of LLMs.How I use LLMs is a more practical guide to examples of use in my own life.Intro to Large Language Models is a third, parallel, more optional video from a longer time ago. 总结glasp ai总结 翻译 概念CoT通过简单的 “请一步步思考再回答” 这样的提示，模型的表现就会大幅提升，这就是 提示词工程 的典型应用。 让模型使用工具解决数学问题Q: 9.11和9.9哪个大(use code)A: 9.9 fine web一个公开的爬虫项目，爬取互联网的英文文本 fineweb common crawl一个共享的爬虫项目 common...</div></div></div></a><a class="pagination-related" href="/2025/04/15/geektime-llm-resource/" title="llm课程资料 from geektime"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">llm课程资料 from geektime</div></div><div class="info-2"><div class="info-item-1">  ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/03/07/deep-dive-into-llms-from-karpathy/" title="笔记:深入了解LLMs"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-07</div><div class="info-item-2">笔记:深入了解LLMs</div></div><div class="info-2"><div class="info-item-1">介绍llm的一些入门扫盲知识，🤓️ai大神karpathy总结的比较全面，看完有些收获记录下。视频地址：Deep Dive into LLMs like ChatGPT is on under-the hood fundamentals of LLMs.How I use LLMs is a more practical guide to examples of use in my own life.Intro to Large Language Models is a third, parallel, more optional video from a longer time ago. 总结glasp ai总结 翻译 概念CoT通过简单的 “请一步步思考再回答” 这样的提示，模型的表现就会大幅提升，这就是 提示词工程 的典型应用。 让模型使用工具解决数学问题Q: 9.11和9.9哪个大(use code)A: 9.9 fine web一个公开的爬虫项目，爬取互联网的英文文本 fineweb common crawl一个共享的爬虫项目 common...</div></div></div></a><a class="pagination-related" href="/2025/04/15/geektime-llm-resource/" title="llm课程资料 from geektime"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-15</div><div class="info-item-2">llm课程资料 from geektime</div></div><div class="info-2"><div class="info-item-1">  ...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">maxwell</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Huggingface"><span class="toc-number">1.</span> <span class="toc-text">Huggingface</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87"><span class="toc-number">1.1.</span> <span class="toc-text">学习目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#transformers%E4%BD%BF%E7%94%A8"><span class="toc-number">1.2.</span> <span class="toc-text">transformers使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB"><span class="toc-number">1.3.</span> <span class="toc-text">文本分类</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MATH"><span class="toc-number">2.</span> <span class="toc-text">MATH</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B5%84%E6%96%99"><span class="toc-number">2.1.</span> <span class="toc-text">资料</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PROMPT-ENGINEERING"><span class="toc-number">3.</span> <span class="toc-text">PROMPT ENGINEERING</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B5%84%E6%96%99-1"><span class="toc-number">3.1.</span> <span class="toc-text">资料</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#paper"><span class="toc-number">3.1.1.</span> <span class="toc-text">paper</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RAG"><span class="toc-number">4.</span> <span class="toc-text">RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B5%84%E6%96%99-2"><span class="toc-number">4.1.</span> <span class="toc-text">资料</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E5%85%B7"><span class="toc-number">4.1.1.</span> <span class="toc-text">工具</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#FINE-TUNE"><span class="toc-number">5.</span> <span class="toc-text">FINE-TUNE</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B5%84%E6%96%99google%E2%80%99s-saif"><span class="toc-number">5.1.</span> <span class="toc-text">资料google’s saif</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B7%A5%E5%85%B7-1"><span class="toc-number">5.2.</span> <span class="toc-text">工具</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5"><span class="toc-number">5.3.</span> <span class="toc-text">概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A8%80%E7%96%8F%E5%90%91%E9%87%8F"><span class="toc-number">5.3.1.</span> <span class="toc-text">稀疏向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#One-Hot%E7%BC%96%E7%A0%81"><span class="toc-number">5.3.2.</span> <span class="toc-text">One-Hot编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tokenization"><span class="toc-number">5.3.3.</span> <span class="toc-text">Tokenization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5%EF%BC%88Word-Embedding%EF%BC%89"><span class="toc-number">5.3.4.</span> <span class="toc-text">词嵌入（Word Embedding）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88activation-function%EF%BC%89"><span class="toc-number">5.3.5.</span> <span class="toc-text">激活函数（activation function）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88Gradient-Descent%EF%BC%89"><span class="toc-number">5.3.6.</span> <span class="toc-text">梯度下降（Gradient Descent）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83"><span class="toc-number">5.3.7.</span> <span class="toc-text">神经元</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%90%E8%97%8F%E7%8A%B6%E6%80%81%EF%BC%88Hidden-State%EF%BC%89"><span class="toc-number">5.3.8.</span> <span class="toc-text">隐藏状态（Hidden State）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#encoder-decoder"><span class="toc-number">5.3.9.</span> <span class="toc-text">encoder &amp; decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6attention"><span class="toc-number">5.3.10.</span> <span class="toc-text">注意力机制attention</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/29/Thoughts/python%E5%BC%80%E5%8F%91%E7%9A%84%E5%B7%A5%E5%85%B7%E9%93%BE%E4%B9%9F%E5%9C%A8%E8%BF%9B%E5%8C%96/" title="python + rust成为主流的工具链开发范式了吧">python + rust成为主流的工具链开发范式了吧</a><time datetime="2025-05-29T12:37:26.515Z" title="Created 2025-05-29 20:37:26">2025-05-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/29/tools/" title="工具收藏">工具收藏</a><time datetime="2025-05-29T12:20:25.296Z" title="Created 2025-05-29 20:20:25">2025-05-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/29/try-to-use-typescript/" title="前端开发">前端开发</a><time datetime="2025-05-29T12:20:25.296Z" title="Created 2025-05-29 20:20:25">2025-05-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/23/ai-security/" title="AI Security">AI Security</a><time datetime="2025-04-23T03:53:07.000Z" title="Created 2025-04-23 11:53:07">2025-04-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/15/geektime-llm-resource/" title="llm课程资料 from geektime">llm课程资料 from geektime</a><time datetime="2025-04-15T08:22:23.000Z" title="Created 2025-04-15 16:22:23">2025-04-15</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/images/footer.jpg);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By maxwell</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>